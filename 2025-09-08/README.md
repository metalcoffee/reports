# Реферат по статье [Magnus Karlsson and Björn Töpel. 2018. The Path to DPDK Speeds for AF XDP.](http://oldvger.kernel.org/lpc_net2018_talks/lpc18_paper_af_xdp_perf-v2.pdf)

## Цитирование
*Как выбранная статья используется в оригинальной статье?
Например, авторы вдохновлялись идеями из нее (и какими), или они подкрепляют ей какое-то утверждение?
Одно-два предложения максимум. Не забудьте также вставить ссылку на выбранную статью.*

В статье AF XDP упоминается как один из подходов к достижению zero-copy операций для приложений, которые могут позволить достичь пропускной способности ∼100Gbps на ядро, но при этом
AF XDP требует переимплементации всей сетевой и транспортной протоколов в пользовательском пространстве (что конечно не радует).
Авторы рассматривают AF XDP скорее как пример существующих решений и потенциал достижения производительности,
но есть нюанс, ведь необходимость переделать все сетевые и транспортные протоколы еще указывает на необходимость большего количества исследований в области zero-copy механизмов
, причем желательно без существенных переделываний приложений или сетевых протоколов.

## Предметная область и постановка задачи
*О чем статья? Какая задача решается? Какие требования предъявляются к решению?*

Статья посвящена оптимизации производительности AF_XDP (Address Family eXpress Data Path) - новый тип сокетов для передачи сырых пакетов из networking cards (NIC) до user space process, введенного в Linux 4.18.

AF XDP предназначен для передачи сетевого трафика от драйвера в ядре непосредственно в пользовательское пространство максимально быстро и эффективно, сохраняя при этом свойства надежности, изоляции и безопасности Linux.

Задача заключается в повышении пропускной способности AF XDP с текущих 20 Mpps до уровней, сопоставимых с пользовательскими драйверами сетевых пакетов, как например тот самый DPDK.

В начальной реализации AF XDP обеспечивал пропускную способность 15-22 Mpps для пакетов размером 64 байта, что было недостаточно для высокопроизводительных приложений

AF XDP работает в трех режимах: skb mode (самый медленный, работает on any NIC), XDP copy mode (который созвучно названию работает on any NIC with XDP support in the driver) и 
zero-copy mode (самый быстрый, работает on XDP enabled drivers в которых был поддержан zero-copy). 

В статье рассматривается только zero-copy mode, потому что он лучший из этих трех.


## Мотивация и актуальность
*Почему эта задача? Чем она важна? В чем проблема существующих решений?*

Существующая реализация AF XDP в Linux 4.18 обеспечивает пропускную способность около 15-22 Mpps для пакетов размером 64 байта,
и это копейки в сравнении с производительностью решений типа DPDK (которые могют в ~70 Mpps), Netmap или PF RING.
Это заметно недостаточно для высокопроизводительных сетевых приложений, особенно с учетом появления сетевых устройств на 40, 100 и 200 Гбит/с.
Авторы статьи все еще надеятся на будущие исследования, но даже сейчас уже заметны удовлетворительные рез-ты "for a number of interesting applications using 40 Gbit/s devices"

Проблема существующих решений заключается в том, что AF_XDP был с самого начал реализован с упором на базовую функциональность,
а не на оптимизацию производительности. Ну а вынужденная необходимость повышения производительности появилась из-за стремлением сделать AF_XDP
удобным решением для приложений с высокой пропускной способностью обработки сетевых пакетов, например мобильная инфра или системы с QoS.
## Подход
*Как авторы решают задачу? В чем заключаются основные идеи?*

Предлагается разбить на два пути оптимизации:

- прозрачные для приложения
- требующие изменения пользовательского API

### Оптимизации Rx:

1. **Встроенный XDP-программный путь**: В теперешней реализации обязательно наличие внешней XDP-программы для маршрутизации пакетов.
     Но Авторы предлагают встроенный XDP путь, который автоматически загружается при использовании флага ```XDP_ATTACH``` в вызове ```bind()```, при этом упрощая конфигурацию и позволяя оптимизировать путь выполнения

3. **Retpoline**: После появления Spectre v2, использование retpoline снизило производительность XDP вдвое.
     Авторы оптимизируют косвенные вызовы функций и заменяют switch на if-операторы в драйвере NIC для улучшения производительности с retpoline

5. **Явный контекст выполнения**: Замена состояния per-cpu на явный контекст в XDP коде, передаваемый между функциями. Он улучшает производительность за
    счет очевидно более эффективного доступа к данным, и сокращения количества поисков в коде XDP

### Оптимизации Tx:

1. **Несколько Tx-сокетов для одного umem**: Поддержка нескольких Tx-сокетов, связанных с одним umem, но с разными queue id.
   Это нужно в частности для поддержки QoS и shaping функций в NIC, где обычно требуется одна очередь на класс

3. **Выделенная HW очередь для AF_XDP**: В стандартной реализации XDP и AF_XDP используют одну HW очередь. Выделение отдельной очереди для AF XDP делает код завершения Tx лучше больше быстрее

4. **Оптимизация размеров батчей и дескрипторов**: Настройка размеров батчей с 16 до 64 и размеров очередей с 1024 до 2048 для соответствия уровням пропускной способности

5. **Режим in-order completion**: Введение нового setsockopt ```XDP_INORDER_COMPLETION```, он позволяет пропустить очередь завершений, если NIC поддерживает упорядоченную передачу пакетов.
   Это устраняет согласованность кэша между ядрами и механизм обратного давления

### еЩЁ оптИмиЗаЦиИ:

**Busy poll() для AF XDP**: Использование busy poll() позволяет выполнять приложение и драйвер на одном ядре, устраняя когерентность кэша между ядрами.
Например в отличие от стандартной модели с двумя ядрами (одно для приложения, другое для ksoftirqd), busy poll() использует одно ядро, на котором приложение управляет NAPI контекстом через вызов poll()

## Вклад и новизна
*В чем заключается вклад работы? Какие результаты получены, в чем их новизна (отличие от предыдущих работ)?*


ной вклад работы заключается в демонстрации достижения производительности AF XDP, близкой к DPDK, через систематическую оптимизацию как Rx, так и Tx путей:

### Результаты оптимизаций:

**Rx**:
- базовая производительность: 15.1 Mpps
- после всех Rx-оптимизаций: 39.3 Mpps (увеличение на 160%)
- в основе этих оптимищзаций основные: встроенны XDP-программный путь и оптимизации retpoline

**Tx**:
- базовая производительность: 25.3 Mpps  
- после всех Tx-оптимизаций: 68.0 Mpps (увеличение на 169%)
- в основе оптимизация размеров батчей и выделение очередей

**Busy poll() оптимизация**:
- стандартная модель (2 ядра): 39.3 Mpps (Rx), 68.0 Mpps (Tx)
- busy poll() модель (1 ядро): 30.4 Mpps (Rx), 51.1 Mpps (Tx)
- на основе одного ядра busy poll() показывает лучшие силы

### Сравнение с DPDK:

сравнение с DPDK:
- AF_XDP в режиме busy poll() достигает ~50% производительности DPDK со скалярными драйверами
- AF_XDP в стандартном режиме превосходит DPDK со скалярными драйверами по Tx, но отстает по Rx ~30%
- для реальных приложений (l2fwd) разница в производительности значительно меньше
- c векторизованными драйверами DPDK разница составляет 16% в худшую сторону для AF_XDP((

### Новизна:

1. **Систематическая оптимизация**: оптимизации как Rx, так и Tx путей AF_XDP
2. **Совместимость с существующими API**: оптимизации прозрачны для приложений
3. **Улучшение пользовательского опыта**: встроенный XDP-программный путь упрощает работу
4. **Поддержка реальных сценариев**: оптимизации для QoS, shaping и in-order
5. **Демонстрация применимости**: оптимизации AF_XDP также улучшают производительность базового XDP на 23%

Эксперименты проводились на системе с двумя процессорами
Intel Broadwell E5-2660 @ 2.7 GHz с 28 ядрами, 64 GB памяти DDR4 @ 2133 MT/s и двумя сетевыми картами Intel I40E 40Gbit/s.

Использовались пакеты размером 64 байта на максимальной линейной скорости 40 Gbit/s.
